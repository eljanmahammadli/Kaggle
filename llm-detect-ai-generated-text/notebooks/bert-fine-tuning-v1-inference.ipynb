{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, sys, re, gc, time, tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom datasets import Dataset\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom torch.nn.parallel import DataParallel\nimport pandas as pd\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:16:24.294152Z","iopub.execute_input":"2023-12-15T02:16:24.294406Z","iopub.status.idle":"2023-12-15T02:16:32.767082Z","shell.execute_reply.started":"2023-12-15T02:16:24.294382Z","shell.execute_reply":"2023-12-15T02:16:32.765726Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# read data depending on whether it is on kaggle, colab or local\nif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n    print(\"Running on Kaggle!\")\n    kernel = 'kaggle'\n    test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n    submission = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n    org_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n    extra = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\nelif \"google.colab\" in sys.modules:\n    print(\"Running on Google Colab!\")\n    kernel = 'google_colab'\n    from google.colab import drive\n    drive.mount('/content/drive')\n    data_path = '/content/drive/MyDrive/Kaggle/LLM_Detect_AI_Generated_Text/data/'\n    test = pd.read_csv(data_path + \"test_essays.csv\")\n    submission = pd.read_csv(data_path + \"sample_submission.csv\")\n    org_train = pd.read_csv(data_path + \"train_essays.csv\")\n    extra = pd.read_csv(data_path + \"train_v2_drcat_02.csv\", sep=\",\")\nelse:\n    print(\"Running locally.\")\n    kernel = 'local'\n    test = pd.read_csv(\"./data/test_essays.csv\")\n    submission = pd.read_csv(\"./data/sample_submission.csv\")\n    org_train = pd.read_csv(\"./data/train_essays.csv\")\n    train = pd.read_csv(\"./data/train_v2_drcat_02.csv\", sep=\",\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:16:50.953024Z","iopub.execute_input":"2023-12-15T02:16:50.954112Z","iopub.status.idle":"2023-12-15T02:16:53.009861Z","shell.execute_reply.started":"2023-12-15T02:16:50.954074Z","shell.execute_reply":"2023-12-15T02:16:53.009104Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Running on Kaggle!\n","output_type":"stream"}]},{"cell_type":"code","source":"# drop duplicates\nextra = extra.drop_duplicates(subset=['text'])\nextra.reset_index(drop=True, inplace=True)\n\n# Text Preprocessing\nstop_words = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n    words = text.split()  # Tokenize\n    words = [word.lower() for word in words if word.isalpha()]  # Lowercase and remove non-alphabetic words\n    words = [word for word in words if word not in stop_words]  # Remove stop words\n    return ' '.join(words)\n\nextra['clean_text'] = extra['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:16:53.140430Z","iopub.execute_input":"2023-12-15T02:16:53.140750Z","iopub.status.idle":"2023-12-15T02:17:03.222120Z","shell.execute_reply.started":"2023-12-15T02:16:53.140723Z","shell.execute_reply":"2023-12-15T02:17:03.221279Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# train, val, test set 90%, 10%, 10% respectively\nextra = extra.sample(frac=1.0, random_state=42).reset_index(drop=True)\nX, y = extra['clean_text'], extra['label']\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\nprint(f\"Train set size: {len(X_train)}\")\nprint(f\"Validation set size: {len(X_val)}\")\nprint(f\"Test set size: {len(X_test)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:17:03.223668Z","iopub.execute_input":"2023-12-15T02:17:03.223960Z","iopub.status.idle":"2023-12-15T02:17:03.293879Z","shell.execute_reply.started":"2023-12-15T02:17:03.223935Z","shell.execute_reply":"2023-12-15T02:17:03.292991Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Train set size: 35894\nValidation set size: 4487\nTest set size: 4487\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"eljanmahammadli/bert-base-uncased-llm-detect-ai\")\nmodel = BertForSequenceClassification.from_pretrained(\"eljanmahammadli/bert-base-uncased-llm-detect-ai\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:17:03.295159Z","iopub.execute_input":"2023-12-15T02:17:03.295512Z","iopub.status.idle":"2023-12-15T02:17:10.658630Z","shell.execute_reply.started":"2023-12-15T02:17:03.295476Z","shell.execute_reply":"2023-12-15T02:17:10.657716Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5b5abdae26490e92cd25f1af3dffb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"778e74a6d66a4860886bc0f74676e563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be929d1ec6c54d2cb5e83a81a041b74d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63d7dc7efbb4a3dba4fe6cb504eab03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd4d54202f16497499df8e5fe091529e"}},"metadata":{}},{"name":"stdout","text":"cuda\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\n\n# Assuming your tokenizer and model are already defined and loaded\n\n# Convert test data into a PyTorch Dataset\ntest_encodings = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors='pt')\ntest_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n\n# Create a DataLoader for your test set\nbatch_size = 100  # You can adjust this depending on your GPU memory\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Function to predict in batches\ndef predict(model, dataloader):\n    model.eval()  # Set model to evaluation mode\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm.tqdm(dataloader):\n            input_ids, attention_mask = [b.to(device) for b in batch]\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            batch_predictions = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n            predictions.extend(batch_predictions)\n\n    return predictions\n\n# Generate predictions\npredictions = predict(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:17:10.661106Z","iopub.execute_input":"2023-12-15T02:17:10.661483Z","iopub.status.idle":"2023-12-15T02:20:01.308002Z","shell.execute_reply.started":"2023-12-15T02:17:10.661440Z","shell.execute_reply":"2023-12-15T02:20:01.307199Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ntest_auc_roc = roc_auc_score(y_test, predictions)\nprint(f\"AUC-ROC on Validation Data: {test_auc_roc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:20:01.309185Z","iopub.execute_input":"2023-12-15T02:20:01.309472Z","iopub.status.idle":"2023-12-15T02:20:01.322847Z","shell.execute_reply.started":"2023-12-15T02:20:01.309447Z","shell.execute_reply":"2023-12-15T02:20:01.321944Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"AUC-ROC on Validation Data: 0.9998\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np\n\n# Convert probabilities to class predictions\n# Assuming predictions is a list of probabilities for the positive class\nthreshold = 0.5\nclass_predictions = [1 if prob > threshold else 0 for prob in predictions]\n\n# Convert y_test to a list if it is not already\ny_test_list = list(y_test)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(y_test_list, class_predictions)\n\n# Calculate the classification report\nclass_report = classification_report(y_test_list, class_predictions)\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\nprint(\"\\nClassification Report:\")\nprint(class_report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T02:20:01.323843Z","iopub.execute_input":"2023-12-15T02:20:01.324162Z","iopub.status.idle":"2023-12-15T02:20:01.365209Z","shell.execute_reply.started":"2023-12-15T02:20:01.324138Z","shell.execute_reply":"2023-12-15T02:20:01.364320Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n[[2718   19]\n [  11 1739]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99      2737\n           1       0.99      0.99      0.99      1750\n\n    accuracy                           0.99      4487\n   macro avg       0.99      0.99      0.99      4487\nweighted avg       0.99      0.99      0.99      4487\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test data processing\ntest_inputs = tokenizer(test['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n\n# Move input tensor to the same device as the model\ntest_inputs = {key: value.to(device) for key, value in test_inputs.items()}\n\n# Generate predictions using your trained model\nwith torch.no_grad():\n    outputs = model(**test_inputs)\n    logits = outputs.logits\n\n# Assuming the first column of logits corresponds to the negative class (non-AI-generated)\n# and the second column corresponds to the positive class (AI-generated)\npredictions = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # Move predictions back to CPU\n\n# Create a submission DataFrame with essay IDs and corresponding predictions\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'generated': predictions\n})\n\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T01:41:58.509180Z","iopub.execute_input":"2023-12-15T01:41:58.510174Z","iopub.status.idle":"2023-12-15T01:41:58.553641Z","shell.execute_reply.started":"2023-12-15T01:41:58.510125Z","shell.execute_reply":"2023-12-15T01:41:58.552502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  del model\n# del test_inputs\n# torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T01:45:52.661463Z","iopub.execute_input":"2023-12-15T01:45:52.662150Z","iopub.status.idle":"2023-12-15T01:45:53.095906Z","shell.execute_reply.started":"2023-12-15T01:45:52.662114Z","shell.execute_reply":"2023-12-15T01:45:53.094742Z"},"trusted":true},"execution_count":null,"outputs":[]}]}