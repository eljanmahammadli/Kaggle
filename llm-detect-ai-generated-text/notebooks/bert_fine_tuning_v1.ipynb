{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T01:34:05.367044Z","iopub.status.busy":"2023-12-14T01:34:05.366088Z","iopub.status.idle":"2023-12-14T01:34:13.886146Z","shell.execute_reply":"2023-12-14T01:34:13.885086Z","shell.execute_reply.started":"2023-12-14T01:34:05.367004Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset, random_split"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T01:25:58.970391Z","iopub.status.busy":"2023-12-14T01:25:58.969962Z","iopub.status.idle":"2023-12-14T01:26:00.243209Z","shell.execute_reply":"2023-12-14T01:26:00.241869Z","shell.execute_reply.started":"2023-12-14T01:25:58.970341Z"},"trusted":true},"outputs":[],"source":["test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n","submission = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n","org_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n","extra = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T01:26:00.246645Z","iopub.status.busy":"2023-12-14T01:26:00.245742Z","iopub.status.idle":"2023-12-14T01:26:08.531984Z","shell.execute_reply":"2023-12-14T01:26:08.530959Z","shell.execute_reply.started":"2023-12-14T01:26:00.246600Z"},"trusted":true},"outputs":[],"source":["# drop duplicates\n","extra = extra.drop_duplicates(subset=['text'])\n","extra.reset_index(drop=True, inplace=True)\n","\n","# Text Preprocessing\n","stop_words = set(stopwords.words('english'))\n","\n","def clean_text(text):\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n","    words = text.split()  # Tokenize\n","    words = [word.lower() for word in words if word.isalpha()]  # Lowercase and remove non-alphabetic words\n","    words = [word for word in words if word not in stop_words]  # Remove stop words\n","    return ' '.join(words)\n","\n","extra['clean_text'] = extra['text'].apply(clean_text)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T01:26:08.533491Z","iopub.status.busy":"2023-12-14T01:26:08.533153Z","iopub.status.idle":"2023-12-14T01:26:08.605988Z","shell.execute_reply":"2023-12-14T01:26:08.604921Z","shell.execute_reply.started":"2023-12-14T01:26:08.533461Z"},"trusted":true},"outputs":[],"source":["# split into train, test, and val set\n","extra = extra.sample(frac=1.0, random_state=42).reset_index(drop=True)\n","train, val = train_test_split(extra, test_size=0.2, random_state=42, stratify=extra['label'])\n","X_train, X_val, y_train, y_val = train_test_split(extra['clean_text'], extra['label'], test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T01:26:27.364536Z","iopub.status.busy":"2023-12-14T01:26:27.363956Z","iopub.status.idle":"2023-12-14T01:26:30.777791Z","shell.execute_reply":"2023-12-14T01:26:30.776650Z","shell.execute_reply.started":"2023-12-14T01:26:27.364500Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71bb475cf118402cbd2c6e9857bc1b2b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97eafdcd75ba4ace82c530ceeb650ba1","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98d3d18b820f4d26b7d2c5d8e3682f51","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d38a980aa4244e3ebb0fc0aefbcce81b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Tokenization and Encoding for BERT\n","tokenizer = BertTokenizer.from_pretrained(\n","    'bert-base-uncased', do_lower_case=True, padding=True, truncation=True, max_length=128\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-14T01:26:44.391763Z","iopub.status.busy":"2023-12-14T01:26:44.391344Z"},"trusted":true},"outputs":[],"source":["encoded_train = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='pt')\n","encoded_val = tokenizer(X_val.tolist(), padding=True, truncation=True, return_tensors='pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Convert labels to tensors\n","train_labels = torch.tensor(y_train.values)\n","val_labels = torch.tensor(y_val.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create TensorDatasets\n","train_dataset = TensorDataset(encoded_train['input_ids'], encoded_train['attention_mask'], train_labels)\n","val_dataset = TensorDataset(encoded_val['input_ids'], encoded_val['attention_mask'], val_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# DataLoader for efficient processing\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=3e-4, correct_bias=False)\n","epochs = 10"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping to avoid exploding gradients\n","        optimizer.step()\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_train_loss:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Validation loop\n","model.eval()\n","val_preds = []\n","val_labels = []\n","\n","with torch.no_grad():\n","    for batch in val_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n","        val_labels.extend(labels.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate validation accuracy\n","val_accuracy = accuracy_score(val_labels, val_preds)\n","print(f\"Validation Accuracy: {val_accuracy:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test data processing\n","test_inputs = tokenizer(test_essays['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n","\n","# Move input tensor to the same device as the model\n","test_inputs = {key: value.to(device) for key, value in test_inputs.items()}\n","\n","# Generate predictions using your trained model\n","with torch.no_grad():\n","    outputs = model(**test_inputs)\n","    logits = outputs.logits\n","\n","# Assuming the first column of logits corresponds to the negative class (non-AI-generated) \n","# and the second column corresponds to the positive class (AI-generated)\n","predictions = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # Move predictions back to CPU\n","\n","# Create a submission DataFrame with essay IDs and corresponding predictions\n","submission = pd.DataFrame({\n","    'id': test_essays['id'],\n","    'generated': predictions\n","})\n","\n","# Save the submission DataFrame to a CSV file\n","submission.to_csv('/kaggle/working/submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
